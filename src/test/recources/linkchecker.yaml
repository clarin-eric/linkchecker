name: "linkchecker-topology"
  
spouts:
  - id: "spout"
    className: "at.ac.oeaw.acdh.linkchecker.spout.SQLSpout"
    parallelism: 1

bolts:
  - id: "partitioner"
    className: "at.ac.oeaw.acdh.linkchecker.bolt.URLPartitionerBolt"
    parallelism: 10 
  - id: "fetcher"
    className: "at.ac.oeaw.acdh.linkchecker.bolt.FetcherBolt"
    parallelism: 10
  - id: "status"
    className: "at.ac.oeaw.acdh.linkchecker.bolt.StatusUpdaterBolt"
    parallelism: 1

streams:
  - from: "spout"
    to: "partitioner"
    grouping:
      type: SHUFFLE

  - from: "partitioner"
    to: "fetcher"
    grouping:
      type: FIELDS
      args: ["key"]

  - from: "fetcher"
    to: "status"
    grouping:
      type: FIELDS
      args: ["url"]
      streamId: "status"

  - from: "fetcher"
    to: "partitioner"
    grouping:
      type: SHUFFLE
      streamId: "redirect"
      
config: 
  topology.workers: 1
  topology.message.timeout.secs: 3600 #1 hour
  topology.max.spout.pending: 1000
  topology.debug: false

  fetcher.threads.number: 50
  
  # override the JVM parameters for the workers
  topology.worker.childopts: "-Xmx2g -Djava.net.preferIPv4Stack=true"

  # mandatory when using Flux
  topology.kryo.register:
    - com.digitalpebble.stormcrawler.Metadata

  # metadata to transfer to the outlinks
  # used by Fetcher for redirections, sitemapparser, etc...
  # these are also persisted for the parent document (see below)
  # metadata.transfer:
  # - customMetadataName

  # lists the metadata to persist to storage
  # these are not transfered to the outlinks
  metadata.persist:
   - _redirTo
   - error.cause
   - error.source
   - isSitemap
   - isFeed

  http.agent.name: "CLARIN Linkchecker: https://www.clarin.eu/linkchecker"
  http.agent.version: "2.0"
  http.agent.description: "built with StormCrawler Archetype 1.14"
  http.agent.url: "https://www.clarin.eu/linkchecker"
  http.agent.email: "acdh-tech@oeaw.ac.at"
  http.redirectLimit: 5

  #default already 1
  #fetcher.server.delay: 1
  fetcher.server.delay.force: true

  # The maximum number of bytes for returned HTTP response bodies.
  # Set -1 to disable the limit.
  # this is 0 so that we don't download any payload when doing GET requests, it will get trimmed.
  http.content.limit: 0

  # FetcherBolt queue dump => comment out to activate
  # if a file exists on the worker machine with the corresponding port number
  # the FetcherBolt will log the content of its internal queues to the logs
  # fetcherbolt.queue.debug.filepath: "/tmp/fetcher-dump-{port}"

  parsefilters.config.file: "parsefilters.json"
  urlfilters.config.file: "urlfilters.json"

  # revisit a page daily (value in minutes)
  # set it to -1 to never refetch a page
  fetchInterval.default: 1440

  # revisit a page with a fetch error after 2 hours (value in minutes)
  # set it to -1 to never refetch a page
  fetchInterval.fetch.error: 120

  # never revisit a page with an error (or set a value in minutes)
  fetchInterval.error: -1

  # text extraction for JSoupParserBolt
  textextractor.include.pattern:
   - DIV[id="maincontent"]
   - DIV[itemprop="articleBody"]
   - ARTICLE

  textextractor.exclude.tags:
   - STYLE
   - SCRIPT

  # custom fetch interval to be used when a document has the key/value in its metadata
  # and has been fetched successfully (value in minutes)
  # fetchInterval.FETCH_ERROR.isFeed=true: 30
  # fetchInterval.isFeed=true: 10

  # configuration for the classes extending AbstractIndexerBolt
  # indexer.md.filter: "someKey=aValue"
  indexer.url.fieldname: "url"
  indexer.text.fieldname: "content"
  indexer.canonical.name: "canonical"
  indexer.md.mapping:
  - parse.title=title
  - parse.keywords=keywords
  - parse.description=description
  - domain=domain

  #Metrics consumers:
  topology.metrics.consumer.register:
     - class: "org.apache.storm.metric.LoggingMetricsConsumer"
       parallelism.hint: 1
       

#######
# SQL #
#######

  sql.connection:
   url: "jdbc:mysql://clarin-mysql:3306/linkchecker_test?useUnicode=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=UTC&useLocalSessionState=true&rewriteBatchedStatements=true"
   user: "linkchecker"
   password: "3kE5f96662Il"
   rewriteBatchedStatements: "true"
   useBatchMultiSend: "true"
   autoReconnect: "true"
   
  sql.max.urls.per.bucket: 1000
  
  sql.status.table: "urls"
  sql.status.resultTable: "status"
  sql.status.historyTable: "history"
  
  sql.spout.max.results: 1000
  
    # time in secs for which the URLs will be considered for fetching after a ack of fail
  spout.ttl.purgatory: 300
  
  #  #Max time to allow between 2 successive queries to the backend. Value in msecs, default 20000.
  spout.max.delay.queries: 600000

  # Min time (in msecs) to allow between 2 successive queries to SQL
  spout.min.delay.queries: 2000

  # Delay since previous query date (in secs) after which the nextFetchDate value will be reset to the current time
  # Setting this to -1 or a large value means that the ES will cache the results but also that less and less results
  # might be returned.
  spout.reset.fetchdate.after: 120

  sql.metrics.table: "metrics"
  
  sql.index.table: "content"

  #default value 1000, you can change to test
  sql.update.batch.size: 1000        
